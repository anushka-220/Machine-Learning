{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method we followed to clean up the images.\n",
    "1. We took the *phase* image \n",
    "2. Converted it to grayscale\n",
    "3. Increased the contrast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image = cv2.imread('D:\\\\Pranav\\\\Pictures\\\\Saved Pictures\\\\1_amplitude.jpg')\n",
    "image2 = cv2.imread('D:\\\\Pranav\\\\Pictures\\\\Saved Pictures\\\\1_phase.jpg') \n",
    "mask = np.zeros_like(image2)\n",
    "# Convert the image to grayscale\n",
    "gray_amp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray_phase = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# histogram equalisation (increases contrast)\n",
    "#equalized_image_phase = cv2.equalizeHist(gray_phase)\n",
    "\n",
    "#CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "equalized_image_phase = clahe.apply(gray_phase)\n",
    "\n",
    "#\n",
    "\n",
    "#thresholding\n",
    "_, thresholded = cv2.threshold(equalized_image_phase, 130, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "# Apply edge detection using Canny\n",
    "edges = cv2.Canny(gray_amp,200, 300, apertureSize=3)\n",
    "edges2 = cv2.Canny(gray_phase,65,75,apertureSize=3)\n",
    "\n",
    "# Find contours in the binary image\n",
    "#contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# draw contours\n",
    "#cv2.drawContours(mask, contours, -1, (255,255,255), thickness=1)\n",
    "plt.set_cmap(\"grey\")\n",
    "plt.figure(figsize=(21,10))\n",
    "plt.subplot(2,4,1)\n",
    "plt.imshow(equalized_image_phase)\n",
    "#plt.subplot(2,4,2)\n",
    "# plt.imshow(thresholded)\n",
    "# plt.subplot(2,4,3)\n",
    "#plt.imshow(mask)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. First approach\n",
    "* Remove the line noise \n",
    "* Apply thresholding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Second approach\n",
    "* Use K means clustering on contrasted phase image to extract useful items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce line noise\n",
    "plt.figure(figsize=(16,16))\n",
    "f = np.fft.fft2(equalized_image_phase)\n",
    "fshift = np.fft.fftshift(f)\n",
    "magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "plt.subplot(3,2,1)\n",
    "plt.imshow(magnitude_spectrum)\n",
    "\n",
    "rows, cols = equalized_image_phase.shape\n",
    "crow,ccol = rows//2 , cols//2\n",
    "fshift[0:crow-10, ccol-5:ccol+5] = 0\n",
    "fshift[crow+10:, ccol-5:ccol+5] = 0\n",
    "magnitude_spectrum_no_horizontal = 20*np.log(np.abs(fshift))\n",
    "plt.subplot(3,2,2)\n",
    "plt.imshow(magnitude_spectrum_no_horizontal)\n",
    "\n",
    "# get the spatial domain back\n",
    "f_ishift = np.fft.ifftshift(fshift)\n",
    "img_back = np.fft.ifft2(f_ishift)\n",
    "img_back = np.real(img_back)\n",
    "\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.imshow(equalized_image_phase)\n",
    "plt.subplot(3,2,4)\n",
    "plt.imsave('./image.jpg', img_back)\n",
    "denoise = cv2.imread('./image.jpg')\n",
    "mask2 = np.zeros_like(denoise)\n",
    "denoise = cv2.cvtColor(denoise, cv2.COLOR_RGB2GRAY)\n",
    "#denoise = cv2.equalizeHist(gray_phase)\n",
    "blurred_image = cv2.GaussianBlur(denoise, (7,7), 0)\n",
    "plt.imshow(blurred_image)\n",
    "\n",
    "edges3 = cv2.Canny(blurred_image, 50,60, apertureSize=3)\n",
    "_, thresholded = cv2.threshold(denoise, 125, 255, cv2.THRESH_BINARY)\n",
    "plt.subplot(3,2,5)\n",
    "plt.imshow(edges3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'kmeans'\n> Overload resolution failed:\n>  - Can't parse 'criteria' as TermCriteria.Input argument doesn't provide sequence protocol\n>  - Can't parse 'criteria' as TermCriteria.Input argument doesn't provide sequence protocol\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Clustering\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(equalized_image_phase)\n\u001b[1;32m----> 3\u001b[0m clusters, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTERM_CRITERIA_COUNT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKMEANS_RANDOM_CENTERS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'kmeans'\n> Overload resolution failed:\n>  - Can't parse 'criteria' as TermCriteria.Input argument doesn't provide sequence protocol\n>  - Can't parse 'criteria' as TermCriteria.Input argument doesn't provide sequence protocol\n"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "img = np.array(equalized_image_phase)\n",
    "clusters, _ = cv2.kmeans(img, 16, None, cv2.TERM_CRITERIA_COUNT, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
